    0    1    2         3         4    5    6    7
0    1.0  0.0  0.0  0.590909  1.000000  0.5  1.0  1.0
1    1.0  0.0  0.0  0.954545  0.166667  0.5  0.0  1.0
2    1.0  0.0  0.0  0.886364  1.000000  1.0  0.0  1.0
3    1.0  0.0  0.0  0.772727  0.666667  1.0  0.0  1.0
4    0.0  1.0  0.0  0.795455  0.500000  0.5  0.5  1.0
5    0.0  1.0  0.0  0.818182  0.666667  1.0  0.5  1.0
6    0.0  1.0  0.0  0.522727  0.166667  0.0  0.0  1.0
7    0.0  1.0  0.0  0.727273  0.333333  0.0  0.0  1.0
8    0.0  1.0  0.0  0.477273  0.166667  0.0  0.5  1.0
9    0.0  1.0  0.0  0.636364  0.333333  0.0  0.0  1.0
10   0.0  1.0  0.0  0.659091  0.333333  0.5  0.0  1.0
11   0.0  1.0  0.0  0.840909  0.666667  1.0  0.5  1.0
12   0.0  1.0  0.0  0.704545  0.166667  0.5  0.0  1.0
13   0.0  1.0  0.0  0.659091  0.500000  1.0  0.0  1.0
14   0.0  1.0  0.0  0.704545  0.333333  0.5  0.5  1.0
15   0.0  1.0  0.0  0.613636  0.333333  1.0  0.0  1.0
16   0.0  1.0  0.0  0.681818  0.833333  1.0  0.5  1.0
17   0.0  1.0  0.0  0.590909  0.166667  0.0  0.5  1.0
18   0.0  1.0  0.0  0.568182  0.500000  1.0  0.5  1.0
19   0.0  1.0  0.0  1.000000  0.166667  0.0  0.5  1.0
20   0.0  1.0  0.0  0.909091  0.166667  0.0  0.5  1.0
21   0.0  1.0  0.0  0.795455  0.333333  1.0  0.0  1.0
22   0.0  1.0  0.0  0.636364  0.333333  0.0  1.0  1.0
23   0.0  1.0  0.0  0.818182  0.166667  0.0  0.5  1.0
24   0.0  1.0  0.0  0.613636  0.333333  0.5  0.5  1.0
25   0.0  1.0  0.0  0.909091  0.333333  0.0  1.0  1.0
26   0.0  1.0  0.0  0.863636  0.333333  0.0  1.0  1.0
27   0.0  1.0  0.0  0.772727  0.500000  0.0  1.0  1.0
28   0.0  1.0  0.0  0.636364  0.666667  0.5  1.0  1.0
29   0.0  1.0  0.0  0.681818  0.666667  1.0  0.0  1.0
..   ...  ...  ...       ...       ...  ...  ...  ...
218  0.0  0.0  1.0  0.636364  0.166667  0.0  0.5  0.0
219  0.0  0.0  1.0  0.659091  0.333333  0.5  0.5  0.0
220  0.0  0.0  1.0  0.818182  0.333333  0.0  0.5  0.0
221  0.0  0.0  1.0  0.636364  0.333333  1.0  0.0  0.0
222  0.0  0.0  1.0  0.636364  0.333333  0.5  0.0  0.0
223  0.0  0.0  1.0  0.636364  0.166667  0.0  0.0  0.0
224  0.0  0.0  1.0  0.613636  0.333333  0.5  0.0  0.0
225  0.0  0.0  1.0  0.795455  0.333333  0.5  0.0  0.0
226  0.0  0.0  1.0  0.568182  0.166667  0.5  0.0  0.0
227  0.0  0.0  1.0  0.772727  0.166667  0.0  0.0  0.0
228  0.0  0.0  1.0  0.704545  0.333333  0.5  0.0  0.0
229  0.0  0.0  1.0  0.590909  0.333333  0.0  1.0  0.0
230  0.0  0.0  1.0  0.727273  0.166667  0.5  0.0  0.0
231  0.0  0.0  1.0  0.477273  0.166667  0.0  0.0  0.0
232  0.0  0.0  1.0  0.636364  0.500000  1.0  0.0  0.0
233  0.0  0.0  1.0  0.840909  0.500000  0.0  1.0  0.0
234  0.0  0.0  1.0  0.568182  0.166667  0.5  0.0  0.0
235  0.0  0.0  1.0  0.727273  0.166667  0.0  0.0  0.0
236  0.0  0.0  1.0  0.568182  0.166667  0.5  0.0  0.0
237  0.0  0.0  1.0  0.704545  0.166667  0.0  0.0  0.0
238  0.0  0.0  1.0  0.863636  1.000000  0.0  1.0  0.0
239  0.0  0.0  1.0  0.590909  0.333333  0.5  0.5  0.0
240  0.0  0.0  1.0  0.704545  0.166667  0.5  0.0  0.0
241  0.0  0.0  1.0  0.704545  0.333333  0.0  0.5  0.0
242  0.0  0.0  1.0  0.568182  0.166667  0.0  0.5  0.0
243  0.0  0.0  1.0  0.704545  0.166667  0.0  0.5  0.0
244  0.0  0.0  1.0  0.772727  0.166667  0.0  0.0  0.0
245  0.0  0.0  1.0  0.795455  0.333333  1.0  0.0  0.0
246  0.0  0.0  1.0  0.659091  0.166667  0.0  0.5  0.0
247  0.0  0.0  1.0  0.522727  0.166667  0.0  0.5  0.0

[248 rows x 8 columns]
                0           1           2           3           4           5  \
count  248.000000  248.000000  248.000000  248.000000  248.000000  248.000000   
mean     0.048387    0.483871    0.467742    0.716001    0.348790    0.286290   
std      0.215017    0.500750    0.499967    0.119354    0.208584    0.369228   
min      0.000000    0.000000    0.000000    0.477273    0.166667    0.000000   
25%      0.000000    0.000000    0.000000    0.636364    0.166667    0.000000   
50%      0.000000    0.000000    0.000000    0.704545    0.333333    0.000000   
75%      0.000000    1.000000    1.000000    0.801136    0.500000    0.500000   
max      1.000000    1.000000    1.000000    1.000000    1.000000    1.000000   

                6           7  
count  248.000000  248.000000  
mean     0.288306    0.334677  
std      0.366271    0.472832  
min      0.000000    0.000000  
25%      0.000000    0.000000  
50%      0.000000    0.000000  
75%      0.500000    1.000000  
max      1.000000    1.000000  
Regression
================================
[[91  9]
 [32 16]]
Regression TrainSet: Accurarcy 72.30%
================================
[[61  4]
 [23 12]]
Regression Testset: Accurarcy 73.00%
================================
================================
================================
Decision Tree
================================
[[99  1]
 [14 34]]
Decsion Tree TrainSet: Accurarcy 89.86%
================================
[[52 13]
 [26  9]]
Decision Tree Testset: Accurarcy 61.00%
================================
================================
================================
C:\Users\graez\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.
  FutureWarning)
Random Forest
================================
[[100   0]
 [ 48   0]]
Random Forest TrainSet: Accurarcy 67.57%
================================
[[65  0]
 [35  0]]
Random Forest Testset: Accurarcy 65.00%
================================
================================
================================
Xgboost
================================
[[98  2]
 [19 29]]
Xgboost TrainSet: Accurarcy 85.81%
==================================
Xgboost on testset confusion matrix
[[58  7]
 [29  6]]
Xgboost on TestSet: Accuracy 64.00%
==================================
5/5 [==============================] - 0s 2ms/step - loss: 0.5976 - accuracy: 0.6757
Neural Network Trainset: 
accuracy: 67.57%
==================================
==================================
Neural Network on testset confusion matrix
[[65  0]
 [35  0]]
Neural Network on TestSet: Accuracy 65.00%