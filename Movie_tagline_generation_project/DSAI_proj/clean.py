# AUTOGENERATED! DO NOT EDIT! File to edit: 02_clean.ipynb (unless otherwise specified).

__all__ = ['extract_images_threaded', 'drop_col', 'create_splits']

# Internal Cell
from pandas import DataFrame
from sklearn.utils import shuffle
from sklearn.preprocessing import MultiLabelBinarizer
from .extract import *
from .eda import *
import os
import pandas as pd
import json
import concurrent
import requests
import pandas as pd
import numpy as np

# Internal Cell

def req_image(url: str, save_path: str, id_num: int):
    req_url = f"https://image.tmdb.org/t/p/original{url}"
    response = requests.get(req_url)
    if response.status_code == 200:
        fname = os.path.join(save_path, f"{id_num}.jpg")
        with open(fname, "wb") as f:
            f.write(response.content)
            return None
    return id_num

# Cell

def extract_images_threaded(df: DataFrame,
                            cur_path: str,
                            img_type: list,
                            max_threads: int) -> tuple:
    max_threads = max_threads if max_threads < len(df) else len(df)
    problem_ids = []
    for itype in img_type:
        save_path = os.path.join(cur_path, f"{itype}_img")
        os.makedirs(save_path, exist_ok=True)

        with concurrent.futures.ThreadPoolExecutor(max_workers=max_threads) as executor:
            pids = [id_num for cnt, url in enumerate(df[f"{itype}_path"]) if (id_num := executor.submit(req_image, url, save_path, df.iloc[cnt]['id']).result()) is not None]
        problem_ids.extend(pids)
        print(f"{itype} images written successfully!")
    problem_ids = set(problem_ids)
    df = df[~df['id'].isin(problem_ids)]
    return df, problem_ids

# Internal Cell

def split_datetime(df: DataFrame,
                   date_col: str) -> DataFrame:
    df[date_col] = pd.to_datetime(df[date_col], format='%Y-%m-%d')
    df[f"{date_col}_year"] = df[date_col].dt.year
    df[f"{date_col}_month"] = df[date_col].dt.month
    df[f"{date_col}_day"] = df[date_col].dt.day
    df.drop(date_col, inplace=True, axis=1)
    return df

# Cell

def drop_col(data: DataFrame,
             irrelevant_cols: list) -> DataFrame:
    df = data.drop(irrelevant_cols,axis = 1)
    df = split_datetime(df=df, date_col="release_date")
    df = df.drop(df.columns[0], axis=1)
    return df

# Cell

def create_splits(df: DataFrame,
                  label: str,
                  splits: list,
                  seed: int,
                  keep_missing: bool,
                  save_path: str = "."):

    assert len(splits) == 2, "Train, validation and test splits must be provided, please provide 2 of them as fractions."
    if keep_missing:
        unlabelled_df = df[df[label] == '']
        unlabelled_df.to_csv(os.path.join(save_path, "tagless.csv"))
        print(f"Tagless set size: {len(unlabelled_df)}")
        print("Tagless dataset created!")
    labelled_df = df[df[label] != '']
    df_size = len(labelled_df)
    labelled_df = shuffle(labelled_df, random_state=seed)
    labelled_df.reset_index(inplace=True, drop=True)
    valid_start, test_start = int(df_size*splits[0]), int(df_size*splits[0] + df_size*splits[1])
    train_df = labelled_df.iloc[:valid_start]
    valid_df = labelled_df.iloc[valid_start:test_start]
    test_df = labelled_df[test_start:]
    print(f"Train set size: {len(train_df)}\nValid set size: {len(valid_df)}\nTest set size: {len(test_df)}")
    train_df.to_csv(os.path.join(save_path, "train.csv"), index=False)
    valid_df.to_csv(os.path.join(save_path, "valid.csv"), index=False)
    test_df.to_csv(os.path.join(save_path, "test.csv"), index=False)
    print("Train, Validation and Test datasets created!")