{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy\n",
    "from sklearn import linear_model\n",
    "from sklearn import preprocessing\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            0         1         2         3         4    5    6    7    8   \\\n",
      "0     0.256080  0.000000  0.000000  0.003663  1.000000  0.0  1.0  0.0  0.0   \n",
      "1     0.127325  0.052795  0.020785  0.000000  1.000000  0.0  1.0  0.0  0.0   \n",
      "2     0.007153  0.059006  0.014627  0.003663  1.000000  0.0  1.0  0.0  0.0   \n",
      "3     0.000000  0.040373  0.010778  0.029304  0.967123  0.0  1.0  0.0  0.0   \n",
      "4     0.000000  0.065217  0.016166  0.029304  0.972603  0.0  1.0  0.0  0.0   \n",
      "5     0.000000  0.118012  0.028483  0.029304  0.947945  0.0  1.0  0.0  0.0   \n",
      "6     0.000000  0.074534  0.018476  0.029304  0.471233  0.0  1.0  0.0  0.0   \n",
      "7     0.127325  0.537267  0.143957  0.010989  0.161644  0.0  1.0  0.0  0.0   \n",
      "8     0.127325  0.611801  0.159353  0.010989  0.364384  0.0  1.0  0.0  0.0   \n",
      "9     0.127325  0.729814  0.193995  0.010989  0.402740  0.0  1.0  0.0  0.0   \n",
      "10    0.020029  0.052795  0.016936  0.113553  0.906849  0.0  1.0  0.0  0.0   \n",
      "11    0.041488  0.027950  0.007698  0.113553  0.756164  0.0  1.0  0.0  0.0   \n",
      "12    0.001431  0.034161  0.010008  0.029304  0.654795  0.0  1.0  0.0  0.0   \n",
      "13    0.001431  0.385093  0.103156  0.018315  0.920548  0.0  1.0  0.0  0.0   \n",
      "14    0.041488  0.037267  0.010778  0.113553  0.931507  0.0  1.0  0.0  0.0   \n",
      "15    0.041488  0.027950  0.010008  0.113553  0.906849  0.0  1.0  0.0  0.0   \n",
      "16    0.127325  0.006211  0.002309  0.113553  0.989041  0.0  1.0  0.0  0.0   \n",
      "17    0.001431  0.136646  0.037721  0.029304  0.978082  0.0  1.0  0.0  0.0   \n",
      "18    0.001431  0.257764  0.089299  0.018315  0.931507  0.0  1.0  0.0  0.0   \n",
      "19    0.001431  0.484472  0.133949  0.018315  0.956164  0.0  1.0  0.0  0.0   \n",
      "20    0.127325  0.080745  0.026944  0.000000  0.997260  0.0  1.0  0.0  0.0   \n",
      "21    0.000000  0.046584  0.013857  0.029304  0.821918  0.0  1.0  0.0  0.0   \n",
      "22    0.001431  0.068323  0.020015  0.007326  0.945205  0.0  1.0  0.0  0.0   \n",
      "23    0.004292  0.031056  0.009238  0.000000  0.991781  1.0  0.0  0.0  0.0   \n",
      "24    0.127325  0.083851  0.024634  0.000000  0.246575  0.0  1.0  0.0  0.0   \n",
      "25    0.001431  0.304348  0.097768  0.000000  0.019178  1.0  0.0  0.0  0.0   \n",
      "26    0.001431  0.208075  0.063125  0.014652  0.457534  0.0  1.0  0.0  0.0   \n",
      "27    0.000000  0.108696  0.033102  0.000000  0.000000  0.0  1.0  0.0  0.0   \n",
      "28    0.130186  0.145963  0.046189  0.000000  0.490411  0.0  1.0  0.0  0.0   \n",
      "29    0.127325  0.003106  0.000770  0.406593  1.000000  0.0  1.0  0.0  0.0   \n",
      "...        ...       ...       ...       ...       ...  ...  ...  ...  ...   \n",
      "5119  0.002861  0.000000  0.076212  0.109890  0.084932  1.0  0.0  0.0  0.0   \n",
      "5120  0.004292  0.000000  0.076212  0.025641  0.180822  1.0  0.0  0.0  0.0   \n",
      "5121  0.001431  0.000000  0.076212  0.003663  0.326027  1.0  0.0  0.0  0.0   \n",
      "5122  0.002861  0.000000  0.076212  0.003663  0.191781  1.0  0.0  0.0  0.0   \n",
      "5123  0.002861  0.000000  0.076212  0.120879  0.997260  1.0  0.0  0.0  0.0   \n",
      "5124  0.000000  0.000000  0.076212  0.047619  0.980822  0.0  1.0  0.0  0.0   \n",
      "5125  0.004292  0.000000  0.076212  0.000000  0.052055  0.0  1.0  0.0  0.0   \n",
      "5126  0.001431  0.000000  0.076212  0.000000  0.068493  0.0  1.0  0.0  0.0   \n",
      "5127  0.002861  0.000000  0.076212  0.003663  0.068493  1.0  0.0  0.0  0.0   \n",
      "5128  0.000000  0.000000  0.076212  0.000000  0.893151  0.0  1.0  0.0  0.0   \n",
      "5129  0.000000  0.000000  0.076212  0.000000  0.936986  1.0  0.0  0.0  0.0   \n",
      "5130  0.000000  0.009317  0.307159  0.003663  0.542466  0.0  1.0  0.0  0.0   \n",
      "5131  0.002861  0.000000  0.076212  0.021978  0.994521  0.0  1.0  0.0  0.0   \n",
      "5132  0.000000  0.000000  0.076212  0.003663  0.849315  0.0  1.0  0.0  0.0   \n",
      "5133  0.000000  0.003106  0.153195  0.003663  0.997260  0.0  1.0  0.0  0.0   \n",
      "5134  0.001431  0.003106  0.153195  0.054945  0.457534  1.0  0.0  0.0  0.0   \n",
      "5135  0.001431  0.000000  0.076212  0.003663  0.202740  0.0  1.0  0.0  0.0   \n",
      "5136  0.002861  0.009317  0.307159  0.000000  0.093151  1.0  0.0  0.0  0.0   \n",
      "5137  0.002861  0.000000  0.076212  0.512821  0.967123  1.0  0.0  0.0  0.0   \n",
      "5138  0.000000  0.003106  0.153195  0.000000  0.821918  0.0  1.0  0.0  0.0   \n",
      "5139  0.000000  0.000000  0.076212  0.025641  0.542466  0.0  0.0  1.0  0.0   \n",
      "5140  0.000000  0.000000  0.076212  0.025641  0.517808  0.0  0.0  1.0  0.0   \n",
      "5141  0.001431  0.000000  0.076212  0.000000  1.000000  0.0  1.0  0.0  0.0   \n",
      "5142  0.002861  0.000000  0.076212  0.120879  0.997260  1.0  0.0  0.0  0.0   \n",
      "5143  0.000000  0.000000  0.076212  0.025641  0.331507  0.0  0.0  1.0  0.0   \n",
      "5144  0.002861  0.015528  0.461124  0.051282  0.517808  1.0  0.0  0.0  0.0   \n",
      "5145  0.000000  0.000000  0.076212  0.000000  0.002740  0.0  1.0  0.0  0.0   \n",
      "5146  0.000000  0.000000  0.076212  0.000000  0.328767  0.0  1.0  0.0  0.0   \n",
      "5147  0.008584  0.000000  0.076212  0.040293  0.435616  1.0  0.0  0.0  0.0   \n",
      "5148  0.000000  0.000000  0.076212  0.000000  0.816438  0.0  1.0  0.0  0.0   \n",
      "\n",
      "       9   ...   46   47   48   49   50   51   52   53   54      55  \n",
      "0     0.0  ...  0.0  0.0  1.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0083  \n",
      "1     0.0  ...  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0081  \n",
      "2     0.0  ...  0.0  0.0  1.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0069  \n",
      "3     0.0  ...  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0206  \n",
      "4     0.0  ...  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0094  \n",
      "5     0.0  ...  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0104  \n",
      "6     0.0  ...  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0208  \n",
      "7     1.0  ...  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0050  \n",
      "8     1.0  ...  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0054  \n",
      "9     1.0  ...  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0042  \n",
      "10    0.0  ...  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0044  \n",
      "11    0.0  ...  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0040  \n",
      "12    0.0  ...  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0417  \n",
      "13    0.0  ...  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0065  \n",
      "14    0.0  ...  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0044  \n",
      "15    0.0  ...  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0040  \n",
      "16    0.0  ...  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0031  \n",
      "17    0.0  ...  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0049  \n",
      "18    0.0  ...  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0060  \n",
      "19    0.0  ...  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0060  \n",
      "20    0.0  ...  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0056  \n",
      "21    0.0  ...  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0104  \n",
      "22    0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0053  \n",
      "23    0.0  ...  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0276  \n",
      "24    0.0  ...  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0100  \n",
      "25    1.0  ...  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0176  \n",
      "26    0.0  ...  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0165  \n",
      "27    0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0044  \n",
      "28    0.0  ...  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0051  \n",
      "29    0.0  ...  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0044  \n",
      "...   ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...     ...  \n",
      "5119  0.0  ...  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0281  \n",
      "5120  0.0  ...  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0189  \n",
      "5121  0.0  ...  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0219  \n",
      "5122  0.0  ...  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0108  \n",
      "5123  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0356  \n",
      "5124  0.0  ...  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0128  \n",
      "5125  0.0  ...  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0431  \n",
      "5126  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0060  \n",
      "5127  0.0  ...  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0300  \n",
      "5128  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0033  \n",
      "5129  0.0  ...  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0264  \n",
      "5130  0.0  ...  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0167  \n",
      "5131  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0035  \n",
      "5132  0.0  ...  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0207  \n",
      "5133  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0096  \n",
      "5134  0.0  ...  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0135  \n",
      "5135  1.0  ...  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0060  \n",
      "5136  0.0  ...  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0014  \n",
      "5137  0.0  ...  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0258  \n",
      "5138  0.0  ...  0.0  0.0  0.0  1.0  0.0  0.0  1.0  0.0  0.0  0.0131  \n",
      "5139  0.0  ...  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0025  \n",
      "5140  0.0  ...  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0026  \n",
      "5141  0.0  ...  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0169  \n",
      "5142  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0344  \n",
      "5143  0.0  ...  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0056  \n",
      "5144  0.0  ...  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0699  \n",
      "5145  0.0  ...  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0060  \n",
      "5146  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0056  \n",
      "5147  0.0  ...  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0237  \n",
      "5148  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0060  \n",
      "\n",
      "[5149 rows x 56 columns]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"listings (unchanged).csv\")\n",
    "df = df.dropna()\n",
    "df1 = pd.get_dummies(df['room_type'])\n",
    "# df2 = pd.concat([df,df1],axis = 1)\n",
    "df3 = pd.get_dummies(df['neighbourhood'])\n",
    "df4 = pd.get_dummies(df['neighbourhood_group'])\n",
    "#df3.head()\n",
    "df = df.drop(columns = ['room_type','neighbourhood','neighbourhood_group'])\n",
    "#df.head()\n",
    "df3 = pd.concat([df,df1,df3,df4],axis=1)\n",
    "# df3.head()\n",
    "dfinal = pd.concat([df3.drop(columns='price'),df['price']],axis = 1)\n",
    "x = dfinal.values\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "x_scaled = min_max_scaler.fit_transform(x)\n",
    "df = pd.DataFrame(x_scaled)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Ce1il03S6_Te",
    "outputId": "a2fa7d02-3221-4c15-ee7c-98d67e078d82",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.19814237 -0.44999918  0.50888312 ... -0.15227109 -0.19929191\n",
      "  -0.25312378]\n",
      " [-0.25916548 -0.53616627 -0.79012011 ... -0.15227109 -0.19929191\n",
      "  -0.25312378]\n",
      " [-0.28967704 -0.53616627 -0.75168807 ... -0.15227109 -0.19929191\n",
      "  -0.25312378]\n",
      " ...\n",
      " [ 0.53413494 -0.47872154 -0.54415501 ... -0.15227109 -0.19929191\n",
      "  -0.25312378]\n",
      " [-0.16763082 -0.277665    0.95469488 ...  6.56723469 -0.19929191\n",
      "  -0.25312378]\n",
      " [-0.28967704 -0.5074439  -0.76706089 ... -0.15227109 -0.19929191\n",
      "  -0.25312378]]\n",
      "[[-0.32018859 -0.33510973 -0.6210191  ... -0.15227109 -0.19929191\n",
      "  -0.25312378]\n",
      " [-0.32018859 -0.07660846 -0.39811322 ... -0.15227109 -0.19929191\n",
      "  -0.25312378]\n",
      " [-0.32018859 -0.42127682 -0.47497732 ... -0.15227109 -0.19929191\n",
      "  -0.25312378]\n",
      " ...\n",
      " [ 0.04595007 -0.53616627 -0.03685197 ... -0.15227109 -0.19929191\n",
      "  -0.25312378]\n",
      " [ 0.16799629 -0.53616627 -0.65176474 ... -0.15227109 -0.19929191\n",
      "  -0.25312378]\n",
      " [-0.35070015  0.52656115 -0.19058016 ... -0.15227109 -0.19929191\n",
      "  -0.25312378]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\graez\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unknown label type: 'continuous'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-48-80b2f59960be>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscaled_X_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[0mlinear_classifier\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlinear_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLogisticRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m123\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m \u001b[0mlinear_classifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscaled_X_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m \u001b[0my_pred_train1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlinear_classifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscaled_X_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[0mcm1_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_pred_train1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1531\u001b[0m         X, y = check_X_y(X, y, accept_sparse='csr', dtype=_dtype, order=\"C\",\n\u001b[0;32m   1532\u001b[0m                          accept_large_sparse=solver != 'liblinear')\n\u001b[1;32m-> 1533\u001b[1;33m         \u001b[0mcheck_classification_targets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1534\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1535\u001b[0m         \u001b[0mn_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\multiclass.py\u001b[0m in \u001b[0;36mcheck_classification_targets\u001b[1;34m(y)\u001b[0m\n\u001b[0;32m    167\u001b[0m     if y_type not in ['binary', 'multiclass', 'multiclass-multioutput',\n\u001b[0;32m    168\u001b[0m                       'multilabel-indicator', 'multilabel-sequences']:\n\u001b[1;32m--> 169\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Unknown label type: %r\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0my_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    170\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Unknown label type: 'continuous'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "Y_position = 55\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)\n",
    "# load pima indians dataset\n",
    "# dataset = numpy.loadtxt(\"mini_fraud_normalised.csv\", delimiter=\",\",skiprows=1)\n",
    "# split into input (X) and output (Y) variables\n",
    "\n",
    "# df = pd.DataFrame(dataset)\n",
    "# print(df)\n",
    "\t# summary statistics\n",
    "# print(df.describe())\n",
    "\n",
    "X = df.iloc[:,0:Y_position]\n",
    "Y = df.iloc[:,Y_position]\n",
    "# create model\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.40, random_state=2020)\n",
    "\n",
    "#scaling to around -2 to 2 (Z)\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "scaled_X_train = scaler.transform(X_train)\n",
    "scaled_X_test = scaler.transform(X_test)\n",
    "\n",
    "#Model 1 : linear regression\n",
    "#https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "#class sklearn.linear_model.LogisticRegression(penalty='l2', *, dual=False, tol=0.0001, C=1.0, fit_intercept=True, \n",
    "#intercept_scaling=1, class_weight=None, random_state=None, solver='lbfgs', max_iter=100, multi_class='auto', \n",
    "#verbose=0, warm_start=False, n_jobs=None, l1_ratio=None)\n",
    "\n",
    "print(scaled_X_train)\n",
    "print(scaled_X_test)\n",
    "linear_classifier = linear_model.LogisticRegression(random_state=123)\n",
    "linear_classifier.fit(scaled_X_train, y_train)\n",
    "y_pred_train1 = linear_classifier.predict(scaled_X_train)\n",
    "cm1_train = confusion_matrix(y_train,y_pred_train1)\n",
    "print(\"Regression\")\n",
    "print(\"================================\")\n",
    "print(cm1_train)\n",
    "acc_train1 = (cm1_train[0,0] + cm1_train[1,1]) / sum(sum(cm1_train))\n",
    "print(\"Regression TrainSet: Accurarcy %.2f%%\" % (acc_train1*100))\n",
    "print(\"================================\")\n",
    "y_pred1 = linear_classifier.predict(scaled_X_test)\n",
    "cm1 = confusion_matrix(y_test,y_pred1)\n",
    "print(cm1)\n",
    "acc1 = (cm1[0,0] + cm1[1,1]) / sum(sum(cm1))\n",
    "print(\"Regression Testset: Accurarcy %.2f%%\" % (acc1*100))\n",
    "print(\"================================\")\n",
    "print(\"================================\")\n",
    "print(\"================================\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model 2: decision tree\n",
    "#https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\n",
    "#class sklearn.tree.DecisionTreeClassifier(*, criterion='gini', splitter='best', max_depth=None, min_samples_split=2, \n",
    "#min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, \n",
    "#min_impurity_decrease=0.0, min_impurity_split=None, class_weight=None, presort='deprecated', ccp_alpha=0.0)\n",
    "\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(scaled_X_train, y_train)\n",
    "y_pred_train2 = clf.predict(scaled_X_train)\n",
    "cm2_train = confusion_matrix(y_train,y_pred_train2)\n",
    "print(\"Decision Tree\")\n",
    "print(\"================================\")\n",
    "print(cm2_train)\n",
    "acc_train2 = (cm2_train[0,0] + cm2_train[1,1]) / sum(sum(cm2_train))\n",
    "print(\"Decsion Tree TrainSet: Accurarcy %.2f%%\" % (acc_train2*100))\n",
    "print(\"================================\")\n",
    "y_pred2 = clf.predict(scaled_X_test)\n",
    "cm2 = confusion_matrix(y_test,y_pred2)\n",
    "acc2 = (cm2[0,0] + cm2[1,1]) / sum(sum(cm2))\n",
    "print(cm2)\n",
    "print(\"Decision Tree Testset: Accurarcy %.2f%%\" % (acc2*100))\n",
    "print(\"================================\")\n",
    "print(\"================================\")\n",
    "print(\"================================\")\n",
    "\n",
    "\n",
    "#Model 3 random forest\n",
    "#https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    "#class sklearn.ensemble.RandomForestClassifier(n_estimators=100, *, criterion='gini', max_depth=None, \n",
    "#min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', \n",
    "#max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, bootstrap=True, oob_score=False, \n",
    "#n_jobs=None, random_state=None, verbose=0, warm_start=False, class_weight=None, ccp_alpha=0.0, max_samples=None)[source]\n",
    "\n",
    "model3 = RandomForestClassifier(n_estimators=100, max_depth=2,random_state=0)\n",
    "model3.fit(scaled_X_train, y_train)\n",
    "y_predicted3 = model3.predict(scaled_X_test)\n",
    "\n",
    "y_pred_train3 = model3.predict(scaled_X_train)\n",
    "cm3_train = confusion_matrix(y_train,y_pred_train3)\n",
    "print(\"Random Forest\")\n",
    "print(\"================================\")\n",
    "print(cm3_train)\n",
    "acc_train3 = (cm3_train[0,0] + cm3_train[1,1]) / sum(sum(cm3_train))\n",
    "print(\"Random Forest TrainSet: Accurarcy %.2f%%\" % (acc_train3*100))\n",
    "print(\"================================\")\n",
    "y_pred3 = model3.predict(scaled_X_test)\n",
    "cm_test3 = confusion_matrix(y_test,y_pred3)\n",
    "print(cm_test3)\n",
    "acc_test3 = (cm_test3[0,0] + cm_test3[1,1]) / sum(sum(cm_test3))\n",
    "print(\"Random Forest Testset: Accurarcy %.2f%%\" % (acc_test3*100))\n",
    "print(\"================================\")\n",
    "print(\"================================\")\n",
    "print(\"================================\")\n",
    "\n",
    "#Model 4: XGBoost\n",
    "\n",
    "print(\"Xgboost\")\n",
    "print(\"================================\")\n",
    "#class sklearn.ensemble.GradientBoostingClassifier(*, loss='deviance', learning_rate=0.1, n_estimators=100, \n",
    "#subsample=1.0, criterion='friedman_mse', min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, \n",
    "#max_depth=3, min_impurity_decrease=0.0, min_impurity_split=None, init=None, random_state=None, max_features=None, \n",
    "#verbose=0, max_leaf_nodes=None, warm_start=False, presort='deprecated', validation_fraction=0.1, \n",
    "#n_iter_no_change=None, tol=0.0001, ccp_alpha=0.0)[source]\n",
    "#https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html\n",
    "\n",
    "model4 = GradientBoostingClassifier(random_state=0)\n",
    "model4.fit(scaled_X_train, y_train)\n",
    "y_pred_train4 = model4.predict(scaled_X_train)\n",
    "cm4_train = confusion_matrix(y_train,y_pred_train4)\n",
    "print(cm4_train)\n",
    "acc_train4 = (cm4_train[0,0] + cm4_train[1,1]) / sum(sum(cm4_train))\n",
    "print(\"Xgboost TrainSet: Accurarcy %.2f%%\" % (acc_train4*100))\n",
    "predictions = model4.predict(scaled_X_test)\n",
    "y_pred4 = (predictions > 0.5)\n",
    "y_pred4 =y_pred4*1 #convert to 0,1 instead of True False\n",
    "cm4 = confusion_matrix(y_test, y_pred4)\n",
    "print(\"==================================\")\n",
    "print(\"Xgboost on testset confusion matrix\")\n",
    "print(cm4)\n",
    "acc4 = (cm4[0,0] + cm4[1,1]) / sum(sum(cm4))\n",
    "print(\"Xgboost on TestSet: Accuracy %.2f%%\" % (acc4*100))\n",
    "print(\"==================================\")\n",
    "\n",
    "#Model 5: neural network\n",
    "#https://www.tensorflow.org/guide/keras/train_and_evaluate\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(10, input_dim=Y_position, activation='relu'))\n",
    "#model.add(Dense(10, activation='relu'))\n",
    "#model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# Compile mode\n",
    "# https://www.tensorflow.org/guide/keras/train_and_evaluate\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='Adamax', metrics=['accuracy'])\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=5, verbose=0)\n",
    "# evaluate the model\n",
    "scores = model.evaluate(X_train, y_train)\n",
    "#print(scores)\n",
    "print(\"Neural Network Trainset: \\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "\n",
    "predictions5 = model.predict(X_test)\n",
    "#print(predictions)\n",
    "#print('predictions shape:', predictions.shape)\n",
    "\n",
    "y_pred5 = (predictions5 > 0.5)\n",
    "y_pred5 = y_pred5*1 #convert to 0,1 instead of True False\n",
    "cm5 = confusion_matrix(y_test, y_pred5)\n",
    "print(\"==================================\")\n",
    "print(\"==================================\")\n",
    "print(\"Neural Network on testset confusion matrix\")\n",
    "print(cm5)\n",
    "\n",
    "## Get accurary from Confusion matrix\n",
    "## Position 0,0 and 1,1 are the correct predictions \n",
    "acc5 = (cm5[0,0] + cm5[1,1]) / sum(sum(cm5))\n",
    "print(\"Neural Network on TestSet: Accuracy %.2f%%\" % (acc5*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     1  0  0.1  0.590909091  0.136363636  0.022727273  0.045454545  1.1  \\\n",
      "0    1  0    0     0.954545     0.022727     0.022727     0.000000    1   \n",
      "1    1  0    0     0.886364     0.136364     0.045455     0.000000    1   \n",
      "2    1  0    0     0.772727     0.090909     0.045455     0.000000    1   \n",
      "3    0  1    0     0.795455     0.068182     0.022727     0.022727    1   \n",
      "4    0  1    0     0.818182     0.090909     0.045455     0.022727    1   \n",
      "5    0  1    0     0.522727     0.022727     0.000000     0.000000    1   \n",
      "6    0  1    0     0.727273     0.045455     0.000000     0.000000    1   \n",
      "7    0  1    0     0.477273     0.022727     0.000000     0.022727    1   \n",
      "8    0  1    0     0.636364     0.045455     0.000000     0.000000    1   \n",
      "9    0  1    0     0.659091     0.045455     0.022727     0.000000    1   \n",
      "10   0  1    0     0.840909     0.090909     0.045455     0.022727    1   \n",
      "11   0  1    0     0.704545     0.022727     0.022727     0.000000    1   \n",
      "12   0  1    0     0.659091     0.068182     0.045455     0.000000    1   \n",
      "13   0  1    0     0.704545     0.045455     0.022727     0.022727    1   \n",
      "14   0  1    0     0.613636     0.045455     0.045455     0.000000    1   \n",
      "15   0  1    0     0.681818     0.113636     0.045455     0.022727    1   \n",
      "16   0  1    0     0.590909     0.022727     0.000000     0.022727    1   \n",
      "17   0  1    0     0.568182     0.068182     0.045455     0.022727    1   \n",
      "18   0  1    0     1.000000     0.022727     0.000000     0.022727    1   \n",
      "19   0  1    0     0.909091     0.022727     0.000000     0.022727    1   \n",
      "20   0  1    0     0.795455     0.045455     0.045455     0.000000    1   \n",
      "21   0  1    0     0.636364     0.045455     0.000000     0.045455    1   \n",
      "22   0  1    0     0.818182     0.022727     0.000000     0.022727    1   \n",
      "23   0  1    0     0.613636     0.045455     0.022727     0.022727    1   \n",
      "24   0  1    0     0.909091     0.045455     0.000000     0.045455    1   \n",
      "25   0  1    0     0.863636     0.045455     0.000000     0.045455    1   \n",
      "26   0  1    0     0.772727     0.068182     0.000000     0.045455    1   \n",
      "27   0  1    0     0.636364     0.090909     0.022727     0.045455    1   \n",
      "28   0  1    0     0.681818     0.090909     0.045455     0.000000    1   \n",
      "29   0  1    0     0.727273     0.022727     0.000000     0.022727    1   \n",
      "..  .. ..  ...          ...          ...          ...          ...  ...   \n",
      "217  0  0    1     0.636364     0.022727     0.000000     0.022727    0   \n",
      "218  0  0    1     0.659091     0.045455     0.022727     0.022727    0   \n",
      "219  0  0    1     0.818182     0.045455     0.000000     0.022727    0   \n",
      "220  0  0    1     0.636364     0.045455     0.045455     0.000000    0   \n",
      "221  0  0    1     0.636364     0.045455     0.022727     0.000000    0   \n",
      "222  0  0    1     0.636364     0.022727     0.000000     0.000000    0   \n",
      "223  0  0    1     0.613636     0.045455     0.022727     0.000000    0   \n",
      "224  0  0    1     0.795455     0.045455     0.022727     0.000000    0   \n",
      "225  0  0    1     0.568182     0.022727     0.022727     0.000000    0   \n",
      "226  0  0    1     0.772727     0.022727     0.000000     0.000000    0   \n",
      "227  0  0    1     0.704545     0.045455     0.022727     0.000000    0   \n",
      "228  0  0    1     0.590909     0.045455     0.000000     0.045455    0   \n",
      "229  0  0    1     0.727273     0.022727     0.022727     0.000000    0   \n",
      "230  0  0    1     0.477273     0.022727     0.000000     0.000000    0   \n",
      "231  0  0    1     0.636364     0.068182     0.045455     0.000000    0   \n",
      "232  0  0    1     0.840909     0.068182     0.000000     0.045455    0   \n",
      "233  0  0    1     0.568182     0.022727     0.022727     0.000000    0   \n",
      "234  0  0    1     0.727273     0.022727     0.000000     0.000000    0   \n",
      "235  0  0    1     0.568182     0.022727     0.022727     0.000000    0   \n",
      "236  0  0    1     0.704545     0.022727     0.000000     0.000000    0   \n",
      "237  0  0    1     0.863636     0.136364     0.000000     0.045455    0   \n",
      "238  0  0    1     0.590909     0.045455     0.022727     0.022727    0   \n",
      "239  0  0    1     0.704545     0.022727     0.022727     0.000000    0   \n",
      "240  0  0    1     0.704545     0.045455     0.000000     0.022727    0   \n",
      "241  0  0    1     0.568182     0.022727     0.000000     0.022727    0   \n",
      "242  0  0    1     0.704545     0.022727     0.000000     0.022727    0   \n",
      "243  0  0    1     0.772727     0.022727     0.000000     0.000000    0   \n",
      "244  0  0    1     0.795455     0.045455     0.045455     0.000000    0   \n",
      "245  0  0    1     0.659091     0.022727     0.000000     0.022727    0   \n",
      "246  0  0    1     0.522727     0.022727     0.000000     0.022727    0   \n",
      "\n",
      "     Unnamed: 8  Unnamed: 9  Unnamed: 10  Unnamed: 11  Unnamed: 12  \\\n",
      "0           NaN         NaN          NaN          NaN          NaN   \n",
      "1           NaN         NaN          NaN          NaN          NaN   \n",
      "2           NaN         NaN          NaN          NaN          NaN   \n",
      "3           NaN         NaN          NaN          NaN          NaN   \n",
      "4           NaN         NaN          NaN          NaN          NaN   \n",
      "5           NaN         NaN          NaN          NaN          NaN   \n",
      "6           NaN         NaN          NaN          NaN          NaN   \n",
      "7           NaN         NaN          NaN          NaN          NaN   \n",
      "8           NaN         NaN          NaN          NaN          NaN   \n",
      "9           NaN         NaN          NaN          NaN          NaN   \n",
      "10          NaN         NaN          NaN          NaN          NaN   \n",
      "11          NaN         NaN          NaN          NaN          NaN   \n",
      "12          NaN         NaN          NaN          NaN          NaN   \n",
      "13          NaN         NaN          NaN          NaN          NaN   \n",
      "14          NaN         NaN          NaN          NaN          NaN   \n",
      "15          NaN         NaN          NaN          NaN          NaN   \n",
      "16          NaN         NaN          NaN          NaN          NaN   \n",
      "17          NaN         NaN          NaN          NaN          NaN   \n",
      "18          NaN         NaN          NaN          NaN          NaN   \n",
      "19          NaN         NaN          NaN          NaN          NaN   \n",
      "20          NaN         NaN          NaN          NaN          NaN   \n",
      "21          NaN         NaN          NaN          NaN          NaN   \n",
      "22          NaN         NaN          NaN          NaN          NaN   \n",
      "23          NaN         NaN          NaN          NaN          NaN   \n",
      "24          NaN         NaN          NaN          NaN          NaN   \n",
      "25          NaN         NaN          NaN          NaN          NaN   \n",
      "26          NaN         NaN          NaN          NaN          NaN   \n",
      "27          NaN         NaN          NaN          NaN          NaN   \n",
      "28          NaN         NaN          NaN          NaN          NaN   \n",
      "29          NaN         NaN          NaN          NaN          NaN   \n",
      "..          ...         ...          ...          ...          ...   \n",
      "217         NaN         NaN          NaN          NaN          NaN   \n",
      "218         NaN         NaN          NaN          NaN          NaN   \n",
      "219         NaN         NaN          NaN          NaN          NaN   \n",
      "220         NaN         NaN          NaN          NaN          NaN   \n",
      "221         NaN         NaN          NaN          NaN          NaN   \n",
      "222         NaN         NaN          NaN          NaN          NaN   \n",
      "223         NaN         NaN          NaN          NaN          NaN   \n",
      "224         NaN         NaN          NaN          NaN          NaN   \n",
      "225         NaN         NaN          NaN          NaN          NaN   \n",
      "226         NaN         NaN          NaN          NaN          NaN   \n",
      "227         NaN         NaN          NaN          NaN          NaN   \n",
      "228         NaN         NaN          NaN          NaN          NaN   \n",
      "229         NaN         NaN          NaN          NaN          NaN   \n",
      "230         NaN         NaN          NaN          NaN          NaN   \n",
      "231         NaN         NaN          NaN          NaN          NaN   \n",
      "232         NaN         NaN          NaN          NaN          NaN   \n",
      "233         NaN         NaN          NaN          NaN          NaN   \n",
      "234         NaN         NaN          NaN          NaN          NaN   \n",
      "235         NaN         NaN          NaN          NaN          NaN   \n",
      "236         NaN         NaN          NaN          NaN          NaN   \n",
      "237         NaN         NaN          NaN          NaN          NaN   \n",
      "238         NaN         NaN          NaN          NaN          NaN   \n",
      "239         NaN         NaN          NaN          NaN          NaN   \n",
      "240         NaN         NaN          NaN          NaN          NaN   \n",
      "241         NaN         NaN          NaN          NaN          NaN   \n",
      "242         NaN         NaN          NaN          NaN          NaN   \n",
      "243         NaN         NaN          NaN          NaN          NaN   \n",
      "244         NaN         NaN          NaN          NaN          NaN   \n",
      "245         NaN         NaN          NaN          NaN          NaN   \n",
      "246         NaN         NaN          NaN          NaN          NaN   \n",
      "\n",
      "     Unnamed: 13  Unnamed: 14  Unnamed: 15  \n",
      "0            NaN          NaN          NaN  \n",
      "1            NaN          NaN          NaN  \n",
      "2            NaN          NaN          NaN  \n",
      "3            NaN          NaN          NaN  \n",
      "4            NaN          NaN          NaN  \n",
      "5            NaN          NaN          NaN  \n",
      "6            NaN          NaN          NaN  \n",
      "7            NaN          NaN          NaN  \n",
      "8            NaN          NaN          NaN  \n",
      "9            NaN          NaN          NaN  \n",
      "10           NaN          NaN          NaN  \n",
      "11           NaN          NaN          NaN  \n",
      "12           NaN          NaN          NaN  \n",
      "13           NaN          NaN          NaN  \n",
      "14           NaN          NaN          NaN  \n",
      "15           NaN          NaN          NaN  \n",
      "16           NaN          NaN          NaN  \n",
      "17           NaN          NaN          NaN  \n",
      "18           NaN          NaN          NaN  \n",
      "19           NaN          NaN          NaN  \n",
      "20           NaN          NaN          NaN  \n",
      "21           NaN          NaN          NaN  \n",
      "22           NaN          NaN          NaN  \n",
      "23           NaN          NaN          NaN  \n",
      "24           NaN          NaN          NaN  \n",
      "25           NaN          NaN          NaN  \n",
      "26           NaN          NaN          NaN  \n",
      "27           NaN          NaN          NaN  \n",
      "28           NaN          NaN          NaN  \n",
      "29           NaN          NaN          NaN  \n",
      "..           ...          ...          ...  \n",
      "217          NaN          NaN          NaN  \n",
      "218          NaN          NaN          NaN  \n",
      "219          NaN          NaN          NaN  \n",
      "220          NaN          NaN          NaN  \n",
      "221          NaN          NaN          NaN  \n",
      "222          NaN          NaN          NaN  \n",
      "223          NaN          NaN          NaN  \n",
      "224          NaN          NaN          NaN  \n",
      "225          NaN          NaN          NaN  \n",
      "226          NaN          NaN          NaN  \n",
      "227          NaN          NaN          NaN  \n",
      "228          NaN          NaN          NaN  \n",
      "229          NaN          NaN          NaN  \n",
      "230          NaN          NaN          NaN  \n",
      "231          NaN          NaN          NaN  \n",
      "232          NaN          NaN          NaN  \n",
      "233          NaN          NaN          NaN  \n",
      "234          NaN          NaN          NaN  \n",
      "235          NaN          NaN          NaN  \n",
      "236          NaN          NaN          NaN  \n",
      "237          NaN          NaN          NaN  \n",
      "238          NaN          NaN          NaN  \n",
      "239          NaN          NaN          NaN  \n",
      "240          NaN          NaN          NaN  \n",
      "241          NaN          NaN          NaN  \n",
      "242          NaN          NaN          NaN  \n",
      "243          NaN          NaN          NaN  \n",
      "244          NaN          NaN          NaN  \n",
      "245          NaN          NaN          NaN  \n",
      "246          NaN          NaN          NaN  \n",
      "\n",
      "[247 rows x 16 columns]\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_csv(\"infert (Original).csv\", delimiter=\",\")\n",
    "# df = pd.DataFrame(dataset)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3nSZP7j1dIBJ"
   },
   "source": [
    "## Feature importance\n",
    "\n",
    "### Feature importances with forests of trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "whZbWt0hdIBK",
    "outputId": "17d439b3-b9c0-4a25-830d-0798bd18ca67",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature ranking:\n",
      "1. feature (Column index) 0 (0.251700)\n",
      "2. feature (Column index) 5 (0.166189)\n",
      "3. feature (Column index) 2 (0.155442)\n",
      "4. feature (Column index) 4 (0.140685)\n",
      "5. feature (Column index) 6 (0.127159)\n",
      "6. feature (Column index) 7 (0.097337)\n",
      "7. feature (Column index) 8 (0.035582)\n",
      "8. feature (Column index) 3 (0.024000)\n",
      "9. feature (Column index) 1 (0.001905)\n"
     ]
    }
   ],
   "source": [
    "RF = model3\n",
    "importances = RF.feature_importances_\n",
    "std = numpy.std([tree.feature_importances_ for tree in RF.estimators_],\n",
    "             axis=0)\n",
    "indices = numpy.argsort(importances)[::-1]\n",
    "\n",
    "# Print the feature ranking\n",
    "print(\"Feature ranking:\")\n",
    "\n",
    "for f in range(X.shape[1]):\n",
    "    print(\"%d. feature (Column index) %s (%f)\" % (f + 1, indices[f], importances[indices[f]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L9XDQ8cSdIBV"
   },
   "source": [
    "## Permutation feature importance\n",
    "\n",
    "### Apply this to any sklearn model\n",
    "\n",
    "-Permutation importance does not reflect to the intrinsic predictive value of a feature by itself but how important this feature is for a particular model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G6gTnEJrdIBY",
    "outputId": "61fcdd14-b513-4062-891c-9f39564fc50e",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature 1   : 0.129 +/- 0.020\n",
      "Feature 5   : 0.030 +/- 0.012\n"
     ]
    }
   ],
   "source": [
    " from sklearn.inspection import permutation_importance\n",
    "r = permutation_importance(linear_classifier, scaled_X_test, y_test, n_repeats=30, random_state=0)\n",
    "\n",
    "for i in r.importances_mean.argsort()[::-1]:\n",
    "     if r.importances_mean[i] - 2 * r.importances_std[i] > 0:\n",
    "         print(f\"{'Feature '+str(i)+'   : '}\"\n",
    "               f\"{r.importances_mean[i]:.3f}\"\n",
    "               f\" +/- {r.importances_std[i]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BuDxnhWGdIBh"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Classification.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
